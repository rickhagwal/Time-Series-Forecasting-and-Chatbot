{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "def weekend(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.weekday() == 5 or date.weekday() == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "# Load dataset\n",
    "cust_orders = pd.read_csv(\"customerOrdersTest0719.csv\", index_col= 'Master')\n",
    "#cust_orders = cust_orders.drop(cust_orders.columns[[1, 69]], axis=1, inplace=True)\n",
    "cust_orders_head=cust_orders.fillna(0)\n",
    "df = pd.DataFrame(cust_orders_head)\n",
    "df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_transposed['Date'] = grouped.columns\n",
    "#grouped_transposed['weekend'] = grouped_transposed['Date'].apply(weekend)\n",
    "#grouped_transposed.columns\n",
    "#grouped\n",
    "df['Date'] =pd.to_datetime(df.Date)\n",
    "df = df.sort_values('Date')\n",
    "df\n",
    "mask = (df['Date'] < '2019-07-17')\n",
    "#print(df.loc[mask])\n",
    "df_mask = df.loc[mask]\n",
    "df_mask\n",
    "#df_mask.transpose()\n",
    "#df_mask['weekend'] = df_mask['Date'].apply(weekend)\n",
    "#df_mask['weekend']\n",
    "#df.columns\n",
    "#df\n",
    "#df_wo_wknd = df_mask[df_mask.weekend != 1]\n",
    "#df_wo_wknd\n",
    "#df_wo_wknd_re = df_wo_wknd\n",
    "#df_wo_wknd_re = df_wo_wknd_re.drop(['weekend'], axis=1)\n",
    "#df_wo_wknd_re\n",
    "#\n",
    "df_wo_wknd_re = df_mask\n",
    "df_wo_wknd_re = df_wo_wknd_re.reset_index()\n",
    "\n",
    "df_wo_wknd_re = df_wo_wknd_re.set_index('Date')\n",
    "df_wo_wknd_re\n",
    "df_wo_wknd_re_trans = df_wo_wknd_re.transpose()\n",
    "# Splitting Train test data for Master - M100705\n",
    "#train_size = int(len(df_wo_wknd) * 0.8)\n",
    "\n",
    "#test_size = len(df_wo_wknd) - train_size\n",
    "#train = df_wo_wknd[:train_size]\n",
    "#test = df_wo_wknd[train_size:len(df_wo_wknd)]\n",
    "#train_transposed = train.transpose()\n",
    "#test_transposed = test.transpose()\n",
    "#train_transposed\n",
    "#test\n",
    "#df_wo_wknd_re.loc[df_wo_wknd_re['Group'] == 'M109015']\n",
    "#df_wo_wknd.index[0]\n",
    "#df_wo_wknd_re.index[1]\n",
    "df_wo_wknd_M100105 = df_wo_wknd_re.loc[df_wo_wknd_re['Group'] == 'M100405']\n",
    "df_wo_wknd_M100105\n",
    "df_wo_wknd_M100105_tr = df_wo_wknd_M100105.transpose()\n",
    "df_wo_wknd_M100105_tr\n",
    "\n",
    "#cust_orders = df_wo_wknd_M100105_tr.drop(df_wo_wknd_M100105_tr.columns[['2019-07-10', '2019-07-31']], axis=1, inplace=True)\n",
    "new_df_wo_wknd_M100105 = pd.DataFrame(df_wo_wknd_M100105)\n",
    "new_df_wo_wknd_M100105\n",
    "df_wo_wknd_M100105 = df_wo_wknd_M100105.reset_index()\n",
    "df_wo_wknd_M100105 = df_wo_wknd_M100105.drop(['Group'],axis=1)\n",
    "#df_wo_wknd_M100105\n",
    "df_wo_wknd_M100105_tr = df_wo_wknd_M100105.transpose()\n",
    "df_wo_wknd_M100105_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = df_wo_wknd_re['Group']\n",
    "index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_wo_wknd_M100105.groupby(['Date'])['Quantity'].apply(np.sum)\n",
    "grouped\n",
    "#grouped_df = pd.DataFrame(columns =['Brand', 'Price'])\n",
    "grouped_df =pd.DataFrame(grouped,columns= [ 'Quantity'])\n",
    "grouped_df\n",
    "grouped_df.columns =['M100405']\n",
    "grouped_df\n",
    "grouped_df_trans = grouped_df.transpose()\n",
    "grouped_df_trans\n",
    "# Splitting Train test data for Master - M100105\n",
    "train_size = int(len(grouped_df) * 1.0)\n",
    "\n",
    "#test_size = len(grouped_df) - train_size\n",
    "train = grouped_df[:train_size]\n",
    "#test = grouped_df[train_size:len(grouped_df)]\n",
    "train_transposed = train.transpose()\n",
    "#test_transposed = test.transpose()\n",
    "#train_transposed\n",
    "#test\n",
    "#train\n",
    "train['Date'] = train.index\n",
    "#train\n",
    "\n",
    "train['Date'] = train['Date'].astype('datetime64[ns]')\n",
    "train = train.set_index('Date')\n",
    "train\n",
    "\n",
    "#test\n",
    "#test['Date'] = test.index\n",
    "#test\n",
    "\n",
    "#test['Date'] = test['Date'].astype('datetime64[ns]')\n",
    "#test = test.set_index('Date')\n",
    "#test_transposed = test.transpose()\n",
    "\n",
    "\n",
    "#test\n",
    "#print(test['Date'].max())\n",
    "#test['ds'] = test.index\n",
    "#test\n",
    "#test['ds'] = test['ds'].astype('datetime64[ns]')\n",
    "#test_transposed = test.transpose()\n",
    "#test_transposed'\n",
    "#test = test.drop(['Date'],axis=1)\n",
    "train_transposed = train.transpose()\n",
    "#test_transposed = test.transpose()\n",
    "\n",
    "usHolidays = pd.DataFrame({\n",
    "  'holiday': 'usHolidays',\n",
    "  'ds': pd.to_datetime(['2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04',\n",
    "               '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24',\n",
    "               '2016-12-26', '2017-01-02', '2017-01-16', '2017-02-20',\n",
    "               '2017-05-29', '2017-07-04', '2017-09-04', '2017-10-09',\n",
    "               '2017-11-10', '2017-11-23', '2017-12-25', '2018-01-01',\n",
    "               '2018-01-15', '2018-02-19', '2018-05-28', '2018-07-04',\n",
    "               '2018-09-03', '2018-10-08', '2018-11-12', '2018-11-22',\n",
    "               '2018-12-25', '2019-01-01', '2019-01-21', '2019-02-18',\n",
    "               '2019-05-27', '2019-07-04', '2019-09-02', '2019-10-14',\n",
    "               '2019-11-11', '2019-11-28', '2019-12-25','2019-12-31']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "holidays =usHolidays\n",
    "holidays\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "\n",
    "#Visualizing Interactive Plots\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "# Initialize plotly\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def plotly_df(df, title=''):\n",
    "    \"\"\"Visualize all the dataframe columns as line plots.\"\"\"\n",
    "    common_kw = dict(x=df.index, mode='lines')\n",
    "    data = [go.Scatter(y=df[c], name=c, **common_kw) for c in df.columns]\n",
    "    layout = dict(title=title)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "# Initialize plotly\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "columns = train.columns\n",
    "fc_re = pd.DataFrame(columns=columns)\n",
    "pred = []\n",
    "count =1\n",
    "\n",
    "for index in range(len(train_transposed)):\n",
    "    prediction_size =169\n",
    "    new = pd.DataFrame(train_transposed.iloc[index])\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    new['ds'] = train_transposed.columns\n",
    "    new['ds'] = new['ds'].astype('datetime64[ns]')\n",
    "    new.columns = ['y', 'ds']\n",
    "    print('new ',new)\n",
    "    new_table = new[['ds','y']]\n",
    "    \n",
    "    \n",
    "    #print('forecast for ',grouped_re.iloc[index:index+1,index:index])\n",
    "    \n",
    "    # Create a dataframe with holiday, ds columns\n",
    "    #new_table['date'] = new_table['ds']\n",
    "   \n",
    "    new_table.set_index('ds')\n",
    "    \n",
    "    my_model = Prophet(interval_width=0.95, holidays=holidays)\n",
    "    #my_model.add_country_holidays(country_name='US')\n",
    "    \n",
    "    my_model.add_seasonality(name='daily', period=1, fourier_order=4)\n",
    "    \n",
    "    plotly_df(new,'Daily Shipment Seasonality')\n",
    "    print('new_table ',new_table)\n",
    "    my_model.fit(new_table)\n",
    "    #print('my_model.train_holiday_names ',my_model.train_holiday_names)\n",
    "    \n",
    "    future_dates = my_model.make_future_dataframe(periods = prediction_size, freq =\"d\")\n",
    "    print('-------------future_dates------------')\n",
    "    print(future_dates)\n",
    "    future_dates['weekend'] = future_dates['ds'].apply(weekend)\n",
    "    future_dates = future_dates[future_dates.weekend != 1]\n",
    "    \n",
    "    future_dates = future_dates.drop(['weekend'], axis=1)\n",
    "    future_dates = future_dates.reset_index()\n",
    "    print('-------------future_dates after------------')\n",
    "    print(future_dates)\n",
    "    #if(future_dates['ds'] >='2019-07-16'):\n",
    "    #    future_dates['weekend'] = future_dates['ds'].apply(weekend)\n",
    "    #else:\n",
    "    #    future_dates['weekend'] = 0\n",
    "    # predictions\n",
    "    #df_wo_wknd_M100105.index.values[index]\n",
    "    forecast_qty = 'M100405'\n",
    "    forecast = my_model.predict(future_dates)\n",
    "    forecast = round(forecast)\n",
    "    \n",
    "    figure = my_model.plot(forecast)\n",
    "    for changepoint in my_model.changepoints:\n",
    "        plt.axvline(changepoint,ls='--', lw=1)\n",
    "    \n",
    "    # Merge forecasts with given IDs for Model 1\n",
    "    #test_merge = pd.merge(forecast, test, on=['ds'] , how='outer')\n",
    "    test_merge = forecast\n",
    "    cols = ['ds','yhat']\n",
    "    test_new = test_merge[cols]\n",
    "    test_new.yhat[test_new.yhat < 0] = 0\n",
    "    \n",
    "    fc = test_new[['ds', 'yhat']].copy()\n",
    "    fc[forecast_qty] = fc['yhat']\n",
    "    fc =  fc.drop('yhat', axis=1)\n",
    "    fc['ds'] = fc['ds'].astype('datetime64[ns]')\n",
    "    \n",
    "    fc_transposed = fc.transpose()\n",
    "    fc_transposed.columns = fc.index.values\n",
    "    fc['weekend'] = fc['ds'].apply(weekend)\n",
    "    fc = fc[fc.weekend != 1]\n",
    "    \n",
    "    fc = fc.drop(['weekend'], axis=1)\n",
    "    \n",
    "    print('fc ',fc)\n",
    "   \n",
    "    if index == 0:\n",
    "        #Model 1\n",
    "        idx = 0\n",
    "        new_col = fc_transposed.iloc[index] #be a list, a Series, an array or a scalar   \n",
    "        fc_re.insert(loc=idx, column='dates', value=new_col)\n",
    "        fc_re['ds'] = fc_re['dates'].astype('datetime64[ns]')\n",
    "        \n",
    "    fc_re[forecast_qty] = fc[forecast_qty]\n",
    "    \n",
    "    test_new = round(test_new)\n",
    "    test_new_df = test_new[['ds', 'yhat']].copy()\n",
    "    test_new_df['y']=test_new['yhat'] #copy the log-transformed data to another column\n",
    "    \n",
    "    prediction_amount = len(grouped_df)\n",
    "    #my_model.plot(forecast)\n",
    "    my_model.plot_components(forecast)\n",
    "    \n",
    "    fig = my_model.plot(forecast)\n",
    "    a = add_changepoints_to_plot(fig.gca(), my_model, forecast)\n",
    "    print(\"count:::-----------> \",count)\n",
    "    count+=1\n",
    "    \n",
    "print('Forecasting done')\n",
    "print('fc_re ', fc_re)\n",
    "fc_re['weekend'] = fc_re['ds'].apply(weekend)\n",
    "fc_re\n",
    "fc_re = fc_re[fc_re.weekend != 1]\n",
    "fc_re\n",
    "output = fc_re.transpose()\n",
    "output.columns = fc_re['ds']\n",
    "print('--------output.columns-------')\n",
    "print(output.columns)\n",
    "new_header = output.iloc[0] #grab the first row for the header\n",
    "print('--------new_header-------')\n",
    "print(new_header)\n",
    "reoutput = output[1:] #take the data less the header row\n",
    "print('--------reoutput before -------')\n",
    "print(reoutput)\n",
    "reoutput.columns = new_header #set the header row as the df header\n",
    "reoutput = reoutput.drop(['ds'],axis = 0)\n",
    "print('--------reoutput after -------')\n",
    "print(reoutput)\n",
    "reoutput = reoutput.astype('float64')\n",
    "grouped_df_trans.columns = pd.to_datetime(grouped_df_trans.columns)\n",
    "new_values =reoutput.copy()\n",
    "reoutput.update(grouped_df_trans)\n",
    "new_values\n",
    "reoutput = reoutput.drop(['weekend'],axis = 0)\n",
    "reoutput\n",
    "\n",
    "# reoutput has Predicted+ Actual Values\n",
    "#new_values has just Predicted data\n",
    "\n",
    "#reoutput.update(grouped)\n",
    "#reoutput\n",
    "#grouped.columns\n",
    "new_values = new_values.drop(['weekend'],axis = 0)\n",
    "new_values\n",
    "vals = grouped_df_trans.iloc[:,0:]\n",
    "vals.to_dict('series')\n",
    "vals.dtypes\n",
    "dataframe = pd.DataFrame(new_values)\n",
    "dataframe.dtypes\n",
    "\n",
    "diff = dataframe.subtract(vals, axis = 1)\n",
    "diff.dtypes\n",
    "diff =abs(diff)\n",
    "diff\n",
    "# sum over the column axis. \n",
    "sum_absolute_vals = diff.sum(axis = 1, skipna = True)\n",
    "sum_absolute_vals\n",
    "diff\n",
    "new_values_trans = new_values.transpose()\n",
    "new_values_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter('fc_re_forecast_M101105_re.xlsx')\n",
    "fc_re.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values_trans.index = pd.to_datetime(new_values_trans.index)\n",
    "new_values_trans = new_values_trans.resample('1M').sum()\n",
    "new_values_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "new_values_trans['month'] = pd.to_datetime(new_values_trans.index).month\n",
    "new_values_trans['year'] = pd.to_datetime(new_values_trans.index).year\n",
    "new_values_trans['month'] \n",
    "new_values_trans['month'] = new_values_trans['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "new_values_trans\n",
    "#pd.merge(new_values_trans['month'], new_values_trans['year'], on=new_values_trans.index, how='inner')\n",
    "#new_values_trans['month'] + new_values_trans['year']\n",
    "new_values_trans['month'] = new_values_trans['month'].astype(str)\n",
    "new_values_trans['year'] = new_values_trans['year'].astype(str)\n",
    "new_values_trans['Period'] = new_values_trans['month']+ ', ' + new_values_trans['year']\n",
    "new_values_trans['Period']\n",
    "new_values_trans\n",
    "#new_values_trans['period']\n",
    "#pd.merge([new_values_trans['month'].dropna(), new_values_trans['year'].dropna()], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values_trans = new_values_trans.drop(['month','year'], axis=1)\n",
    "new_values_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reoutput.insert(0, \"Error Rate\", mape_error_rate, True) \n",
    "#reoutput\n",
    "writer = ExcelWriter('M101105_forecast_cust_orders.xlsx')\n",
    "new_values_trans.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required from here\n",
    "#Actual Values\n",
    "sum_actual_vals = reoutput.sum(axis = 1, skipna = True)\n",
    "sum_actual_vals\n",
    "\n",
    "#MAPE Calculation\n",
    "mape_error_rate = sum_absolute_vals.div(sum_actual_vals)\n",
    "mape_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reoutput.insert(0, \"Error Rate\", mape_error_rate, True) \n",
    "reoutput\n",
    "writer = ExcelWriter('Customer_orders_Prediction_MD82405_new.xlsx')\n",
    "reoutput.to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
