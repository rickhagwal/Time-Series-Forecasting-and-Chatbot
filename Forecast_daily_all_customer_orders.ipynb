{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "\n",
    "#Visualizing Interactive Plots\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import graph_objs as go\n",
    "from datetime import datetime\n",
    "\n",
    "#today_date\n",
    "\n",
    "# Initialize plotly\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#weekend\n",
    "def weekend(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.weekday() == 5 or date.weekday() == 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#usHolidays\n",
    "usHolidays = pd.DataFrame({\n",
    "  'holiday': 'usHolidays',\n",
    "  'ds': pd.to_datetime(['2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04',\n",
    "               '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24',\n",
    "               '2016-12-26', '2017-01-02', '2017-01-16', '2017-02-20',\n",
    "               '2017-05-29', '2017-07-04', '2017-09-04', '2017-10-09',\n",
    "               '2017-11-10', '2017-11-23', '2017-12-25', '2018-01-01',\n",
    "               '2018-01-15', '2018-02-19', '2018-05-28', '2018-07-04',\n",
    "               '2018-09-03', '2018-10-08', '2018-11-12', '2018-11-22',\n",
    "               '2018-12-25', '2019-01-01', '2019-01-21', '2019-02-18',\n",
    "               '2019-05-27', '2019-07-04', '2019-09-02', '2019-10-14',\n",
    "               '2019-11-11', '2019-11-28', '2019-12-25','2019-12-31']),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 1,\n",
    "})\n",
    "holidays =usHolidays\n",
    "holidays\n",
    "\n",
    "#plotly_df\n",
    "def plotly_df(df, title=''):\n",
    "    \"\"\"Visualize all the dataframe columns as line plots.\"\"\"\n",
    "    common_kw = dict(x=df.index, mode='lines')\n",
    "    data = [go.Scatter(y=df[c], name=c, **common_kw) for c in df.columns]\n",
    "    layout = dict(title=title)\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    iplot(fig, show_link=False)\n",
    "\n",
    "# Load dataset\n",
    "cust_orders = pd.read_csv(\"master_codes_test.csv\", index_col= 'Group')\n",
    "#fill with 0\n",
    "cust_orders_head=cust_orders.fillna(0)\n",
    "#create dataframe\n",
    "df = pd.DataFrame(cust_orders_head)\n",
    "\n",
    "df['Date'] =pd.to_datetime(df.Date)\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "mask = (df['Date'] < today_date)\n",
    "df_mask = df.loc[mask]\n",
    "\n",
    "df_mask['weekend'] = df_mask['Date'].apply(weekend)\n",
    "df_mask['weekend']\n",
    "df_wo_wknd = df_mask[df_mask.weekend != 1]\n",
    "\n",
    "df_wo_wknd_re = df_wo_wknd\n",
    "df_wo_wknd_re = df_wo_wknd_re.drop(['weekend'], axis=1)\n",
    "df_wo_wknd_re = df_wo_wknd_re.reset_index()\n",
    "\n",
    "df_wo_wknd_re = df_wo_wknd_re.set_index('Date')\n",
    "\n",
    "#get unique master/group codes\n",
    "group_code_unique = df_wo_wknd_re['Group'].unique()\n",
    "\n",
    "#get unique dates\n",
    "#date_unique = df_wo_wknd_re.index.unique()\n",
    "\n",
    "#print('date_unique ',date_unique)\n",
    "\n",
    "master_index=0\n",
    "#for each master/group code, genrate loop\n",
    "for master in group_code_unique:\n",
    "    print(master)\n",
    "    df_wo_wknd_ele = df_wo_wknd_re.loc[df_wo_wknd_re['Group'] == master]\n",
    "\n",
    "    df_wo_wknd_ele = df_wo_wknd_ele.reset_index()\n",
    "\n",
    "    df_wo_wknd_ele = df_wo_wknd_ele.drop(['Group'],axis=1)\n",
    "    df_wo_wknd_ele_tr = df_wo_wknd_ele.transpose()\n",
    "    grouped = df_wo_wknd_ele.groupby(['Date'])['Quantity'].apply(np.sum)\n",
    "    grouped_df =pd.DataFrame(grouped,columns= [ 'Quantity'])\n",
    "    grouped_df.columns =[master]\n",
    "    grouped_df_transposed = grouped_df.transpose()\n",
    "    # Splitting Train test data for individual Master codes\n",
    "    train_size = int(len(grouped_df) * 0.9)\n",
    "    test_size = len(grouped_df) - train_size\n",
    "    train = grouped_df[:train_size]\n",
    "    test = grouped_df[train_size:len(grouped_df)]\n",
    "    train['Date'] = train.index\n",
    "    train['Date'] = train['Date'].astype('datetime64[ns]')\n",
    "    train = train.set_index('Date')\n",
    "    test['Date'] = test.index\n",
    "    test['Date'] = test['Date'].astype('datetime64[ns]')\n",
    "    test = test.set_index('Date')\n",
    "    test_transposed = test.transpose()\n",
    "    test['ds'] = test.index\n",
    "    test['ds'] = test['ds'].astype('datetime64[ns]')\n",
    "    train_transposed = train.transpose()\n",
    "    test_transposed = test.transpose()\n",
    "    #print('--------------------------------test_transposed--------------------------------')\n",
    "    #print(test_transposed)\n",
    "    # Prediction size\n",
    "    prediction_size = 300\n",
    "    new = pd.DataFrame(train_transposed.iloc[0])\n",
    "    #ax = plt.gca()\n",
    "    \n",
    "    # create 'ds' and 'y' elements for Prophet ML algo\n",
    "    new['ds'] = train_transposed.columns\n",
    "    new['ds'] = new['ds'].astype('datetime64[ns]')\n",
    "    new.columns = ['y', 'ds']\n",
    "    new_table = new[['ds','y']]\n",
    "    new_table.set_index('ds')\n",
    "    \n",
    "    #create Model by calling Prophet\n",
    "    my_model = Prophet(interval_width=0.95, holidays=holidays)\n",
    "    \n",
    "    #Add Daily Seasonality\n",
    "    my_model.add_seasonality(name='daily', period=1, fourier_order=4)\n",
    "    \n",
    "    #Plot Graph with existing data\n",
    "    #plotly_df(new,'Daily Shipment Seasonality')\n",
    "    \n",
    "    # Fit model on train data\n",
    "    my_model.fit(new_table)\n",
    "    \n",
    "    # Make Future Dataframe with daily frequency and Prediction size\n",
    "    future_dates = my_model.make_future_dataframe(periods = prediction_size, freq =\"d\")\n",
    "    #print('future_dates ',future_dates)\n",
    "    #Predict Future on Test data\n",
    "    forecast = my_model.predict(future_dates)\n",
    "    forecast = round(forecast)\n",
    "    \n",
    "    #Plot Forecast\n",
    "    #figure = my_model.plot(forecast)\n",
    "    #for changepoint in my_model.changepoints:\n",
    "    #    plt.axvline(changepoint,ls='--', lw=1)\n",
    "\n",
    "    # Merge forecasts with given IDs for Model 1\n",
    "    test_merge = pd.merge(forecast, test, on=['ds'] , how='outer')\n",
    "    cols = ['ds','yhat']\n",
    "    test_new = test_merge[cols]\n",
    "    test_new.yhat[test_new.yhat < 0] = 0\n",
    "\n",
    "    # Forecasted data\n",
    "    fc = test_new[['ds', 'yhat']].copy()\n",
    "    fc[master] = fc['yhat']\n",
    "    fc =  fc.drop('yhat', axis=1)\n",
    "    fc['ds'] = fc['ds'].astype('datetime64[ns]')\n",
    "\n",
    "    fc_transposed = fc.transpose()\n",
    "    fc_transposed.columns = fc.index.values\n",
    "    fc['weekend'] = fc['ds'].apply(weekend)\n",
    "    fc = fc[fc.weekend != 1]\n",
    "\n",
    "    fc = fc.drop(['weekend'], axis=1)\n",
    "    print('fc ',fc)\n",
    "    print(fc['ds'])\n",
    "    \n",
    "    if master_index == 0:\n",
    "        idx = 0\n",
    "        columns = train.columns\n",
    "        fc_re = pd.DataFrame(columns=columns)\n",
    "        grouped_df_transposed_re = pd.DataFrame(columns=columns)\n",
    "        new_col = fc_transposed.iloc[master_index] #be a list, a Series, an array or a scalar \n",
    "        \n",
    "        fc_re.insert(loc=idx, column='dates', value=new_col)\n",
    "        fc_re['ds'+master] = fc_re['dates'].astype('datetime64[ns]')\n",
    "        #fc_re.insert(loc=idx, column='ds'+master, value=fc['ds'])\n",
    "        #fc_re['ds'] = fc_re['ds'].astype('datetime64[ns]')\n",
    "    else:\n",
    "        fc_re['ds'+master] = fc['ds']\n",
    "    grouped_df_transposed_re[master] = grouped_df[master]\n",
    "    \n",
    "    \n",
    "    fc_re[master] = fc[master]\n",
    "    \n",
    "    #print(fc_re[master])\n",
    "\n",
    "    #fc_re[master] = fc_re[master].fillna(0)\n",
    "    #fc_re['weekend'+master] = fc_re['ds'+master].apply(weekend)\n",
    "    #fc_re = fc_re[fc_re['weekend'+master] != 1]\n",
    "\n",
    "    test_new = round(test_new)\n",
    "    test_new_df = test_new[['ds', 'yhat']].copy()\n",
    "    test_new_df['y']=test_new['yhat'] #copy the log-transformed data to another column\n",
    "    \n",
    "    #my_model.plot_components(forecast)\n",
    "\n",
    "    #fig = my_model.plot(forecast)\n",
    "    #a = add_changepoints_to_plot(fig.gca(), my_model, forecast)\n",
    "    master_index+=1\n",
    "    print('------------starts----------')\n",
    "    #print('fc_re inside loop ', fc_re)\n",
    "    print(fc_re['ds'+master])\n",
    "    #fc_re = fc_re[fc_re['ds'+master] < pd.datetime(2020, 1, 1)]\n",
    "    print('------------ends----------')\n",
    "print('Forecasting done')\n",
    "#print('fc_re ', fc_re)\n",
    "print('-------------grouped_df_transposed_re--------------------')\n",
    "print(grouped_df_transposed_re)\n",
    "\n",
    "#for master in group_code_unique:\n",
    "#    fc_re['weekend'] = fc_re['ds'+master].apply(weekend)\n",
    "#    fc_re = fc_re[fc_re.weekend != 1]\n",
    "#to add later on\n",
    "#fc_re['weekend'] = fc_re['ds'].apply(weekend)\n",
    "#fc_re\n",
    "#fc_re = fc_re[fc_re.weekend != 1]\n",
    "#fc_re\n",
    "fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for master in group_code_unique:\n",
    "    fc_re = fc_re[fc_re['ds'+master] < pd.datetime(2020, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc_re[(fc_re['dates'].dt.date <'2020-01-01') ]\n",
    "fc_re = fc_re[fc_re['dates'] < pd.datetime(2020, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter('fc_re_forecast_re_with_weekend.xlsx')\n",
    "fc_re.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2017-01-03\n",
       "1      2017-01-04\n",
       "2      2017-01-05\n",
       "3      2017-01-06\n",
       "4      2017-01-07\n",
       "5      2017-01-08\n",
       "6      2017-01-09\n",
       "7      2017-01-10\n",
       "8      2017-01-11\n",
       "9      2017-01-12\n",
       "10     2017-01-13\n",
       "11     2017-01-14\n",
       "12     2017-01-15\n",
       "13     2017-01-16\n",
       "14     2017-01-17\n",
       "15     2017-01-18\n",
       "16     2017-01-19\n",
       "17     2017-01-20\n",
       "18     2017-01-21\n",
       "19     2017-01-22\n",
       "20     2017-01-23\n",
       "21     2017-01-24\n",
       "22     2017-01-25\n",
       "23     2017-01-26\n",
       "24     2017-01-27\n",
       "25     2017-01-28\n",
       "26     2017-01-29\n",
       "27     2017-01-30\n",
       "28     2017-01-31\n",
       "29     2017-02-01\n",
       "          ...    \n",
       "1066   2019-12-05\n",
       "1067   2019-12-06\n",
       "1068   2019-12-07\n",
       "1069   2019-12-08\n",
       "1070   2019-12-09\n",
       "1071   2019-12-10\n",
       "1072   2019-12-11\n",
       "1073   2019-12-12\n",
       "1074   2019-12-13\n",
       "1075   2019-12-14\n",
       "1076   2019-12-15\n",
       "1077   2019-12-16\n",
       "1078   2019-12-17\n",
       "1079   2019-12-18\n",
       "1080   2019-12-19\n",
       "1081   2019-12-20\n",
       "1082   2019-12-21\n",
       "1083   2019-12-22\n",
       "1084   2019-12-23\n",
       "1085   2019-12-24\n",
       "1086   2019-12-25\n",
       "1087   2019-12-26\n",
       "1088   2019-12-27\n",
       "1089   2019-12-28\n",
       "1090   2019-12-29\n",
       "1091   2019-12-30\n",
       "1092   2019-12-31\n",
       "1093   2020-01-01\n",
       "1094   2020-01-02\n",
       "1095   2020-01-03\n",
       "Name: Date, Length: 1096, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta, date\n",
    "import pandas as pd\n",
    "\n",
    "start_date = date(2017, 1, 3)\n",
    "end_date = date(2020, 1, 3)\n",
    "dataframeall = pd.DataFrame()\n",
    "dataframeall = pd.concat([pd.DataFrame({'Date': pd.date_range(start_date, end_date, freq='d')}, columns=['Date'])])\n",
    "dataframeall\n",
    "\n",
    "dataframeall['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fc_re['Dates'] = dataframeall['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeall['weekend'] = dataframeall['Date'].apply(weekend)\n",
    "dataframeall = dataframeall[dataframeall.weekend != 1]\n",
    "dataframeall\n",
    "dataframeall = dataframeall.drop(['weekend'],axis=1)\n",
    "dataframeall\n",
    "dataframeall.index = range(0,len(dataframeall ))\n",
    "dataframeall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for master in group_code_unique:\n",
    "    fc_re['weekend'] = fc_re['ds'+master].apply(weekend)\n",
    "    fc_re\n",
    "    fc_re = fc_re[fc_re.weekend != 1]\n",
    "fc_re\n",
    "#fc_re.index = range(0,len(fc_re ))\n",
    "#fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for master in group_code_unique:\n",
    "    fc_re['weekend'] = fc_re['ds'+master].apply(weekend)\n",
    "    fc_re\n",
    "    fc_re = fc_re[fc_re.weekend != 1]\n",
    "fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re['dsM100915'] = fc_re['dsM100915'].fillna('1990-12-12').astype('datetime64[ns]')\n",
    "fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if fc_re[fc_re.dsM100915.isnull()]:\n",
    "#    fc_re.dsM100915 = 0\n",
    "#print(fc_re)\n",
    "fc_re[pd.notnull(fc_re['dsM100915'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "#fc_re['DateAll'] =  dataframeall\n",
    "#fc_re['DateAll']\n",
    "#fc_re[fc_re['DateAll'].notnull()]\n",
    "fc_re\n",
    "#print(dataframeall['Date'].dt.date)\n",
    "#fc_re['dsMD80105'] = fc_re['dsMD80105'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "dates = pd.to_datetime(pd.Series(dataframeall['Date']), format = '%Y%m%d')\n",
    "dates = dates.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "#rint('dates ',dates)\n",
    "#print(dates.dtype)\n",
    "print(len(group_code_unique))\n",
    "new_re = pd.DataFrame()\n",
    "for k in range(len(group_code_unique)):\n",
    "    #print(k)\n",
    "    fc_re['ds'+group_code_unique[k]] = fc_re['ds'+group_code_unique[k]].fillna('1990-12-12').astype('datetime64[ns]')\n",
    "    fc_redates = pd.to_datetime(pd.Series(fc_re['ds'+group_code_unique[k]]))\n",
    "    fc_redates = fc_redates.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "    #rint(fc_redates)\n",
    "    #print(fc_re['ds'+group_code_unique[k]])\n",
    "    \n",
    "    #new_df_fc_redates['Date'] = dataframeall['Date']\n",
    "    i=0\n",
    "    new_df_fc_redates = pd.DataFrame(columns =['Date',group_code_unique[k]])\n",
    "    for single in dates:\n",
    "        #print(i)\n",
    "        #print(single)\n",
    "        #print('yo')\n",
    "        vals = fc_redates[fc_redates.astype(str).str.contains(single)]\n",
    "        #print(vals)\n",
    "        if (len(vals) == 1):\n",
    "            vals =  pd.to_datetime(vals.values)\n",
    "            value = fc_re.loc[i,group_code_unique[k]]\n",
    "            #print(value)\n",
    "            #new_df_fc_redates.loc[i] = [vals.values.to_pydatetime()] + value\n",
    "            new_df_fc_redates = new_df_fc_redates.append({'Date':vals.values, group_code_unique[k]:value}, ignore_index=True)\n",
    "            #int(new_df_fc_redates)\n",
    "        else:\n",
    "            #print('else')\n",
    "            i=i-1\n",
    "            vals =  pd.to_datetime(vals.values)\n",
    "            #print('else vals',vals)\n",
    "            #value = fc_re.loc[i,group_code_unique[0]]\n",
    "            #value = fc_re.loc[i,group_code_unique[0]]\n",
    "            #print(value)\n",
    "            new_df_fc_redates = new_df_fc_redates.append({'Date':vals.values, group_code_unique[k]:0}, ignore_index=True)\n",
    "            #new_df_fc_redates.loc[i] = [0] + [0]\n",
    "            #rint(new_df_fc_redates)\n",
    "        i+=1\n",
    "    #rint('start')\n",
    "    #rint(new_df_fc_redates)\n",
    "    print(new_df_fc_redates)\n",
    "    new_re[group_code_unique[k]] = new_df_fc_redates[group_code_unique[k]]\n",
    "    #rint('yo')\n",
    "    #rint(new_re[group_code_unique[k]])\n",
    "    #ew_df_fc_redates+=new_df_fc_redates\n",
    "    #rint(new_re)\n",
    "    #rint('end')\n",
    "new_df_fc_redates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_re['ds'] = dataframeall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_df_transposed_re\n",
    "writer = ExcelWriter('new_re.xlsx')\n",
    "new_re.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = dates[dates.astype(str).str.contains(single)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re.loc[fc_re['ds'+master].isin(dataframeall['Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped_df_transposed_re\n",
    "writer = ExcelWriter('grouped_df_transposed_re.xlsx')\n",
    "grouped_df_transposed_re.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_re['weekend'] = fc_re['ds'].apply(weekend)\n",
    "fc_re = fc_re[fc_re.weekend != 1]\n",
    "output = fc_re.transpose()\n",
    "output.columns = fc_re['ds']\n",
    "new_header = output.iloc[0] #grab the first row for the header\n",
    "reoutput = output[1:] #take the data less the header row\n",
    "reoutput.columns = new_header #set the header row as the df header\n",
    "reoutput = reoutput.drop(['ds'],axis = 0)\n",
    "reoutput = reoutput.astype('float64')\n",
    "grouped_df_transposed.columns = pd.to_datetime(grouped_df_transposed.columns)\n",
    "new_values =reoutput.copy()\n",
    "reoutput.update(grouped_df_transposed_re.transpose())\n",
    "reoutput = reoutput.drop(['weekend'],axis = 0)\n",
    "new_values = new_values.drop(['weekend'],axis = 0)\n",
    "vals = grouped_df_transposed_re.iloc[:,0:]\n",
    "vals.to_dict('series')\n",
    "dataframe = pd.DataFrame(new_values)\n",
    "diff = dataframe.subtract(vals.transpose(), axis = 1)\n",
    "diff =abs(diff)\n",
    "# Absolute Values - sum over the column axis. \n",
    "sum_absolute_vals = diff.sum(axis = 1, skipna = True)\n",
    "sum_absolute_vals\n",
    "diff\n",
    "new_values_trans = new_values.transpose()\n",
    "new_values_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_values_trans.set_index('Date', inplace=True)\n",
    "new_values_trans.index = pd.to_datetime(new_values_trans.index)\n",
    "new_values_trans = new_values_trans.resample('1M').sum()\n",
    "new_values_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "new_values_trans['month'] = pd.to_datetime(new_values_trans.index).month\n",
    "new_values_trans['year'] = pd.to_datetime(new_values_trans.index).year\n",
    "new_values_trans['month'] \n",
    "new_values_trans['month'] = new_values_trans['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "new_values_trans\n",
    "#pd.merge(new_values_trans['month'], new_values_trans['year'], on=new_values_trans.index, how='inner')\n",
    "#new_values_trans['month'] + new_values_trans['year']\n",
    "new_values_trans['month'] = new_values_trans['month'].astype(str)\n",
    "new_values_trans['year'] = new_values_trans['year'].astype(str)\n",
    "new_values_trans['Period'] = new_values_trans['month']+ ', ' + new_values_trans['year']\n",
    "new_values_trans['Period']\n",
    "new_values_trans\n",
    "new_values_trans = new_values_trans.drop(['month','year'], axis=1)\n",
    "new_values_trans = new_values_trans.reset_index()\n",
    "new_values_trans = new_values_trans.set_index('Period')\n",
    "new_values_trans\n",
    "new_values_trans = new_values_trans.drop('dates',axis=1)\n",
    "new_values_trans\n",
    "#new_values_trans['period']\n",
    "#pd.merge([new_values_trans['month'].dropna(), new_values_trans['year'].dropna()], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reoutput.insert(0, \"Error Rate\", mape_error_rate, True) \n",
    "#reoutput\n",
    "writer = ExcelWriter('forecast_cust_orders_test.xlsx')\n",
    "new_values_trans.transpose().to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not required\n",
    "reoutput.update(new_values_trans.transpose())\n",
    "writer = ExcelWriter('forecast_cust_orders_test_old_n_new.xlsx')\n",
    "reoutput.to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not required\n",
    "\n",
    "reoutput.update(vals.transpose())\n",
    "\n",
    "#Actual Values\n",
    "sum_actual_vals = reoutput.sum(axis = 1, skipna = True)\n",
    "sum_actual_vals\n",
    "\n",
    "#MAPE Calculation\n",
    "mape_error_rate = sum_absolute_vals.div(sum_actual_vals)\n",
    "mape_error_rate\n",
    "\n",
    "reoutput.insert(0, \"Error Rate\", mape_error_rate, True) \n",
    "writer = ExcelWriter('Customer_orders_Prediction_new_1172019_change.xlsx')\n",
    "reoutput.to_excel(writer,'Sheet1',index=True)\n",
    "writer.save()\n",
    "\n",
    "#new_values.insert(0, \"Error Rate\", mape_error_rate, True) \n",
    "#new_values\n",
    "#writer = ExcelWriter('Customer_orders_Prediction_wo_old_new_1172019_test.xlsx')\n",
    "#new_values.to_excel(writer,'Sheet1',index=True)\n",
    "#writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
